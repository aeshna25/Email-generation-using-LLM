import os
import config
from langchain_groq import ChatGroq
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.exceptions import OutputParserException
from dotenv import load_dotenv

import pandas as pd
import chromadb
import uuid
from uuid import uuid4

import re

import streamlit as st
from langchain_community.document_loaders import WebBaseLoader
import os
import sys

if os.path.isfile("requirement.txt"):
    # Install required packages
    try:
        os.system("pip install -r requirement.txt")
    except Exception as e:
        print(f"Error installing required packages: {e}")
        sys.exit(1)
else:
    print("No requirements file found.")



class Chain:
    def __init__(self):
        self.llm = ChatGroq(temperature=0, groq_api_key=config.GROQ_API_KEY, model_name="llama-3.1-70b-versatile")

    def extract_jobs(self, cleaned_text):
        prompt_extract = PromptTemplate.from_template(
            """
            ### SCRAPED TEXT FROM WEBSITE:
            {page_data}
            ### INSTRUCTION:
            The scraped text is from the career's page of a website.
            Your job is to extract the job postings and return them in JSON format containing the following keys: `role`, `experience`, `skills` and `description`.
            Only return the valid JSON.
            ### VALID JSON (NO PREAMBLE):
            """
        )
        chain_extract = prompt_extract | self.llm
        res = chain_extract.invoke(input={"page_data": cleaned_text})
        try:
            json_parser = JsonOutputParser()
            res = json_parser.parse(res.content)
        except OutputParserException:
            raise OutputParserException("Context too big. Unable to parse jobs.")
        return res if isinstance(res, list) else [res]

    def write_mail(self, job, links):
        prompt_email = PromptTemplate.from_template(
            """
            ### JOB DESCRIPTION:
            {job_description}

            ### INSTRUCTION:
        You are X, a business development executive at ContractAI. ContractAI is an AI & Software Consulting company dedicated to facilitating
        the seamless integration of business processes through automated tools. 
        Over our experience, we have empowered numerous enterprises with tailored solutions, fostering scalability, 
        process optimization, cost reduction, and heightened overall efficiency. 
        Your job is to write a cold email to the client regarding the job mentioned above describing the capability of ContractAI 
        in fulfilling their needs.
        Also add the most relevant ones from the following links to showcase ContractAI's portfolio: {link_list}
        Remember you are Mohan, BDE at ContractAI. 
        Do not provide a preamble.
        ### EMAIL (NO PREAMBLE):

            """
        )
        chain_email = prompt_email | self.llm
        res = chain_email.invoke({"job_description": str(job), "link_list": links})
        return res.content


# chromadb
class Portfolio:
    def __init__(self, file_path=r"app\resource\my_portfolio.csv"):
        self.file_path = file_path
        self.data = pd.read_csv(file_path)
        self.chroma_client = chromadb.PersistentClient('vectorstore')
        self.collection = self.chroma_client.get_or_create_collection(name="portfolio")

    def load_portfolio(self):
        if not self.collection.count():
            for _, row in self.data.iterrows():
                self.collection.add(documents=row["Techstack"],
                                    metadatas={"links": row["Links"]},
                                    ids=[str(uuid.uuid4())])

    def query_links(self, skills):
        return self.collection.query(query_texts=skills, n_results=2).get('metadatas', [])
    



def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<[^>]*?>', '', text)
    # Remove URLs
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    # Remove special characters
    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)
    # Replace multiple spaces with a single space
    text = re.sub(r'\s{2,}', ' ', text)
    # Trim leading and trailing whitespace
    text = text.strip()
    # Remove extra whitespace
    text = ' '.join(text.split())
    return text

def create_streamlit_app(llm, portfolio, clean_text):
    st.title("ðŸ“§ Cold Mail Generator")
    url_input = st.text_input("Enter a URL:", value="https://www.uber.com/careers/list/134286")
    submit_button = st.button("Submit")

    if submit_button:
        try:
            loader = WebBaseLoader([url_input]) #enter the JD link
            data = clean_text(loader.load().pop().page_content) #clean the input(JD link) before processing
            portfolio.load_portfolio() #get vectordb
            jobs = llm.extract_jobs(data) #get json skills, experience , jd
            for job in jobs:
                skills = job.get('skills', []) #get skills key, trigger prompt chain
                links = portfolio.query_links(skills) #get links justifying the JD 
                email = llm.write_mail(job, links) # trigger email chain
                st.code(email, language='markdown')
        except Exception as e:
            st.error(f"An Error Occurred: {e}")


if __name__ == "__main__":
    chain = Chain()
    portfolio = Portfolio()
    st.set_page_config(layout="wide", page_title="Cold Email Generator", page_icon="ðŸ“§")
    create_streamlit_app(chain, portfolio, clean_text)

# run and debug and run it
# then to run python -m streamlit run app\mains.py
